# Prediction Module - LSTM Quantity Predictor

## M√¥ t·∫£
Module n√†y s·ª≠ d·ª•ng m√¥ h√¨nh LSTM ƒë√£ ƒë∆∞·ª£c train ƒë·ªÉ d·ª± ƒëo√°n s·ªë l∆∞·ª£ng (quantity) trong t∆∞∆°ng lai d·ª±a tr√™n d·ªØ li·ªáu l·ªãch s·ª≠.

## C√¥ng ngh·ªá
- **Model Format:** ONNX (Open Neural Network Exchange)
- **Runtime:** ONNX Runtime for Node.js
- **Framework:** NestJS + TypeScript

## üöÄ C√†i ƒë·∫∑t & Setup

### B∆∞·ªõc 1: Export PyTorch Model sang ONNX

```bash
# C√†i ƒë·∫∑t dependencies Python
pip install torch onnx

# Ch·ªânh s·ª≠a config trong export_to_onnx.py (n·∫øu c·∫ßn)
# ƒê·∫£m b·∫£o c√°c tham s·ªë kh·ªõp v·ªõi model ƒë√£ train

# Export model
python export_to_onnx.py
```

**Output:** `src/model/lstm_quantity_predictor.onnx`

### B∆∞·ªõc 2: C√†i ƒë·∫∑t ONNX Runtime

```bash
cd BE_WEB
npm install onnxruntime-node
```

### B∆∞·ªõc 3: Kh·ªüi ƒë·ªông server

```bash
npm run start:dev
```

## üìù C·∫•u h√¨nh Model

Trong file `export_to_onnx.py`, ƒëi·ªÅu ch·ªânh c√°c tham s·ªë ƒë·ªÉ kh·ªõp v·ªõi model ƒë√£ train:

```python
CONFIG = {
    'input_size': 1,        # S·ªë features (univariate = 1)
    'hidden_size': 50,      # LSTM hidden units
    'num_layers': 1,        # S·ªë l·ªõp LSTM
    'sequence_length': 10,  # ƒê·ªô d√†i sequence m·∫∑c ƒë·ªãnh
}
```

## API Endpoints

### 1. Predict Quantity
D·ª± ƒëo√°n s·ªë l∆∞·ª£ng trong t∆∞∆°ng lai d·ª±a tr√™n d·ªØ li·ªáu l·ªãch s·ª≠.

**Endpoint:** `POST /prediction/quantity`

**Headers:**
```
Authorization: Bearer <token>
Content-Type: application/json
```

**Request Body:**
```json
{
  "historicalData": [100, 120, 115, 130, 125, 140, 135],
  "steps": 7
}
```

**Response:**
```json
{
  "success": true,
  "predictions": [145, 150, 148, 155, 152, 158, 160],
  "metadata": {
    "inputLength": 7,
    "steps": 7,
    "min": 100,
    "max": 160
  }
}
```

### 2. Get Model Info
L·∫•y th√¥ng tin v·ªÅ model ƒë√£ load.

**Endpoint:** `GET /prediction/model-info`

**Response:**
```json
{
  "loaded": true,
  "inputShape": [null, null, 1],
  "outputShape": [null, 1],
  "layers": 2,
  "trainable": true
}
```

## Test th·ª≠

### 1. Test v·ªõi Swagger UI
Truy c·∫≠p: `http://localhost:3000/api-docs`
- T√¨m section "prediction"
- Th·ª≠ endpoint POST /prediction/quantity

### 2. Test v·ªõi curl
```bash
# Get model info
curl -X GET http://localhost:3000/prediction/model-info \
  -H "Authorization: Bearer YOUR_TOKEN"

# Make prediction
curl -X POST http://localhost:3000/prediction/quantity \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "historicalData": [100, 120, 115, 130, 125, 140, 135],
    "steps": 7
  }'
```

### 3. Test v·ªõi Postman
Import collection ho·∫∑c t·∫°o request m·ªõi:
- URL: `POST http://localhost:3000/prediction/quantity`
- Headers: `Authorization: Bearer <token>`
- Body (raw JSON):
```json
{
  "historicalData": [100, 120, 115, 130, 125, 140, 135],
  "steps": 7
}
```

## File chuy·ªÉn ƒë·ªïi model (convert_to_onnx.py)

T·∫°o file n√†y ƒë·ªÉ chuy·ªÉn ƒë·ªïi PyTorch model:

```python
import torch
import torch.nn as nn

# Define your LSTM model architecture (ph·∫£i gi·ªëng v·ªõi khi train)
class LSTMPredictor(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=1, output_size=1):
        super(LSTMPredictor, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        predictions = self.fc(lstm_out[:, -1, :])
        return predictions

# Load model
model = LSTMPredictor()
model.load_state_dict(torch.load('src/model/lstm_quantity_predictor.pth'))
model.eval()

# Export to ONNX
dummy_input = torch.randn(1, 7, 1)  # [batch_size, sequence_length, features]
torch.onnx.export(
    model, 
    dummy_input, 
    'lstm_quantity_predictor.onnx',
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': {0: 'batch_size', 1: 'sequence_length'},
        'output': {0: 'batch_size'}
    }
)

print("‚úÖ Model exported to ONNX format!")
```

## L∆∞u √Ω

1. **Dummy Model:** Hi·ªán t·∫°i service ƒëang s·ª≠ d·ª•ng dummy model ƒë·ªÉ test. C·∫ßn chuy·ªÉn ƒë·ªïi model th·∫≠t ƒë·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c.

2. **Input Shape:** Model c·∫ßn input c√≥ shape `[batch_size, sequence_length, features]`
   - batch_size: s·ªë l∆∞·ª£ng sequences (th∆∞·ªùng l√† 1)
   - sequence_length: ƒë·ªô d√†i chu·ªói l·ªãch s·ª≠
   - features: s·ªë features (th∆∞·ªùng l√† 1 cho univariate time series)

3. **Normalization:** Service t·ª± ƒë·ªông normalize data b·∫±ng min-max scaling tr∆∞·ªõc khi predict.

4. **Performance:** TensorFlow.js Node c√≥ performance t·ªët h∆°n TensorFlow.js browser version.

## Troubleshooting

### L·ªói: Module '@tensorflow/tfjs-node' not found
```bash
npm install @tensorflow/tfjs-node
```

### L·ªói: Model load failed
- Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n model trong `prediction.service.ts`
- ƒê·∫£m b·∫£o model ƒë√£ ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi sang TensorFlow.js format

### Prediction kh√¥ng ch√≠nh x√°c
- Ki·ªÉm tra input data c√≥ ƒë√∫ng format kh√¥ng
- ƒê·∫£m b·∫£o normalization ƒë∆∞·ª£c √°p d·ª•ng ƒë√∫ng
- Verify model architecture match v·ªõi l√∫c train
