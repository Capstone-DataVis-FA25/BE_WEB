import { Injectable, Logger, InternalServerErrorException } from '@nestjs/common';
import { PredictQuantityDto } from './dto/predict-quantity.dto';
import * as ort from 'onnxruntime-node';
import * as path from 'path';
import * as fs from 'fs';
import { PredictNextDayDto } from './dto/predict-next-day.dto';
import { ChatPredictDto } from './dto/chat-predict.dto';
import { ConfigService } from '@nestjs/config';

export interface Preprocessor {
  scaler_y_min: number;
  scaler_y_max: number;
}

@Injectable()
export class PredictionService {
  private readonly logger = new Logger(PredictionService.name);
  private session: ort.InferenceSession | null = null;
  private preprocessor: Preprocessor | null = null;
  private readonly modelPath = path.join(process.cwd(), 'src', 'model', 'lstm_quantity_predictor.onnx');
  private readonly preprocessorPath = path.join(process.cwd(), 'src', 'model', 'preprocessor.json');
  private readonly apiKey: string;
  private readonly baseUrl = 'https://openrouter.ai/api/v1';
  private readonly model = 'google/gemini-2.5-flash-lite-preview-09-2025';

  constructor(private readonly configService: ConfigService) {
    this.apiKey = this.configService.get<string>('OPENROUTER_API_KEY') || '';
    this.loadModel();
    this.loadPreprocessor();
  }

  /**
   * Load preprocessor configuration
   */
  private loadPreprocessor() {
    try {
      if (!fs.existsSync(this.preprocessorPath)) {
        this.logger.warn('‚ö†Ô∏è Preprocessor config not found at: ' + this.preprocessorPath);
        return;
      }

      const data = fs.readFileSync(this.preprocessorPath, 'utf-8');
      this.preprocessor = JSON.parse(data);
      this.logger.log('‚úÖ Preprocessor loaded successfully');
      this.logger.log(`   Min: ${this.preprocessor.scaler_y_min}, Max: ${this.preprocessor.scaler_y_max}`);
    } catch (error) {
      this.logger.error('‚ùå Failed to load preprocessor:', error);
    }
  }

  /**
   * Load ONNX model
   */
  private async loadModel() {
    try {
      if (!fs.existsSync(this.modelPath)) {
        this.logger.warn('‚ö†Ô∏è ONNX model not found at: ' + this.modelPath);
        this.logger.warn('Please export your PyTorch model to ONNX format and place it in src/model/');
        return;
      }

      this.session = await ort.InferenceSession.create(this.modelPath);
      this.logger.log('‚úÖ ONNX model loaded successfully');
      this.logger.log(`üìä Model inputs: ${this.session.inputNames.join(', ')}`);
      this.logger.log(`üìä Model outputs: ${this.session.outputNames.join(', ')}`);
      
      // Log input/output shapes for debugging
      const inputMetadata = this.session.inputNames.map(name => {
        const meta = (this.session as any).inputsInfo?.[name];
        return `${name}: ${meta?.dims || 'unknown shape'}`;
      });
      this.logger.log(`üìê Input shapes: ${inputMetadata.join(', ')}`);
      
    } catch (error) {
      this.logger.error('‚ùå Failed to load ONNX model:', error);
    }
  }

  /**
   * Predict future quantities based on historical data using ONNX model
   */
  async predictQuantity(dto: PredictQuantityDto) {
    try {
      if (!this.session) {
        throw new InternalServerErrorException('ONNX model not loaded');
      }

      if (!this.preprocessor) {
        throw new InternalServerErrorException('Preprocessor not loaded');
      }

      const { historicalData, steps = 1 } = dto;
      
      // Normalize data using preprocessor config (min-max scaling)
      const { scaler_y_min, scaler_y_max } = this.preprocessor;
      const range = scaler_y_max - scaler_y_min;
      
      const normalizedData = historicalData.map(val => 
        (val - scaler_y_min) / range
      );
      
      this.logger.log(`üìä Input: ${historicalData.slice(0, 5).join(', ')}${historicalData.length > 5 ? '...' : ''}`);
      this.logger.log(`üìä Normalized: ${normalizedData.slice(0, 5).map(v => v.toFixed(4)).join(', ')}${normalizedData.length > 5 ? '...' : ''}`);
      
      // Make predictions
      const predictions: number[] = [];
      let currentSequence = [...normalizedData];

      // Model expects fixed shape [1, 30, 2]
      const EXPECTED_SEQ_LEN = 30;
      const EXPECTED_FEATURES = 2;

      for (let i = 0; i < steps; i++) {
        // Take last EXPECTED_SEQ_LEN values, pad if needed
        let sequence = currentSequence.slice(-EXPECTED_SEQ_LEN);
        
        // Pad with zeros at the beginning if not enough data
        while (sequence.length < EXPECTED_SEQ_LEN) {
          sequence.unshift(0);
        }

        // Create input with 2 features: [value, normalized_index]
        const inputData: number[] = [];
        for (let j = 0; j < EXPECTED_SEQ_LEN; j++) {
          inputData.push(sequence[j]); // feature 1: normalized value
          inputData.push(j / EXPECTED_SEQ_LEN); // feature 2: position in sequence (0 to 1)
        }

        // Create tensor with shape [1, 30, 2]
        const inputTensor = new ort.Tensor(
          'float32',
          Float32Array.from(inputData),
          [1, EXPECTED_SEQ_LEN, EXPECTED_FEATURES]
        );

        // Run inference
        const feeds: Record<string, ort.Tensor> = {};
        feeds[this.session.inputNames[0]] = inputTensor;
        
        const results = await this.session.run(feeds);
        const output = results[this.session.outputNames[0]];
        
        // Get prediction value (normalized)
        const predValue = output.data[0] as number;
        
        // Denormalize using preprocessor
        const denormalizedValue = predValue * range + scaler_y_min;
        predictions.push(Math.round(denormalizedValue));

        // Update sequence for next prediction (keep normalized)
        currentSequence.push(predValue);
        
        this.logger.debug(`Step ${i + 1}: normalized=${predValue.toFixed(4)}, denormalized=${Math.round(denormalizedValue)}`);
      }

      this.logger.log(`‚úÖ Predictions: ${predictions.join(', ')}`);

      return {
        success: true,
        predictions,
        metadata: {
          inputLength: historicalData.length,
          steps,
          scaler_min: scaler_y_min,
          scaler_max: scaler_y_max,
        },
      };
    } catch (error) {
      this.logger.error('‚ùå Prediction error:', error);
      throw new InternalServerErrorException('Failed to make prediction: ' + error.message);
    }
  }

  /**
   * Predict next day sales quantity based on historical daily data
   */
  async predictNextDay(dto: PredictNextDayDto) {
    try {
      if (!this.session) {
        throw new InternalServerErrorException('ONNX model not loaded');
      }

      if (!this.preprocessor) {
        throw new InternalServerErrorException('Preprocessor not loaded');
      }

      const { dailySales } = dto;
      
      // Sort by date to ensure chronological order
      const sortedSales = [...dailySales].sort((a, b) => 
        new Date(a.date).getTime() - new Date(b.date).getTime()
      );

      // Extract quantities for prediction
      const quantities = sortedSales.map(item => item.quantity);
      
      // Get date range
      const firstDate = new Date(sortedSales[0].date);
      const lastDate = new Date(sortedSales[sortedSales.length - 1].date);
      const nextDate = new Date(lastDate);
      nextDate.setDate(nextDate.getDate() + 1);

      // Calculate trend
      const firstHalf = quantities.slice(0, Math.floor(quantities.length / 2));
      const secondHalf = quantities.slice(Math.floor(quantities.length / 2));
      const firstAvg = firstHalf.reduce((a, b) => a + b, 0) / firstHalf.length;
      const secondAvg = secondHalf.reduce((a, b) => a + b, 0) / secondHalf.length;
      const trendPercent = ((secondAvg - firstAvg) / firstAvg) * 100;
      
      let trend = 'stable';
      if (trendPercent > 5) trend = 'increasing';
      else if (trendPercent < -5) trend = 'decreasing';

      // Normalize data
      const { scaler_y_min, scaler_y_max } = this.preprocessor;
      const range = scaler_y_max - scaler_y_min;
      const normalizedData = quantities.map(val => (val - scaler_y_min) / range);

      // Model expects fixed shape [1, 30, 2]
      const EXPECTED_SEQ_LEN = 30;
      const EXPECTED_FEATURES = 2;

      // Take last EXPECTED_SEQ_LEN values, pad if needed
      let sequence = normalizedData.slice(-EXPECTED_SEQ_LEN);
      
      // Pad with zeros at the beginning if not enough data
      while (sequence.length < EXPECTED_SEQ_LEN) {
        sequence.unshift(0);
      }

      // Create input with 2 features: [value, normalized_index]
      const inputData: number[] = [];
      for (let j = 0; j < EXPECTED_SEQ_LEN; j++) {
        inputData.push(sequence[j]); // feature 1: normalized value
        inputData.push(j / EXPECTED_SEQ_LEN); // feature 2: position in sequence
      }

      // Create tensor with shape [1, 30, 2]
      const inputTensor = new ort.Tensor(
        'float32',
        Float32Array.from(inputData),
        [1, EXPECTED_SEQ_LEN, EXPECTED_FEATURES]
      );

      // Run inference
      const feeds: Record<string, ort.Tensor> = {};
      feeds[this.session.inputNames[0]] = inputTensor;
      
      const results = await this.session.run(feeds);
      const output = results[this.session.outputNames[0]];
      
      // Get prediction (normalized)
      const predValue = output.data[0] as number;
      
      // Denormalize
      const prediction = Math.round(predValue * range + scaler_y_min);

      // Calculate confidence based on data variance
      const mean = quantities.reduce((a, b) => a + b, 0) / quantities.length;
      const variance = quantities.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / quantities.length;
      const stdDev = Math.sqrt(variance);
      const coefficientOfVariation = (stdDev / mean) * 100;
      
      let confidence = 'high';
      if (coefficientOfVariation > 30) confidence = 'low';
      else if (coefficientOfVariation > 15) confidence = 'medium';

      this.logger.log(`üìÖ Predicting for: ${nextDate.toISOString().split('T')[0]}`);
      this.logger.log(`üìä Input period: ${sortedSales[0].date} to ${sortedSales[sortedSales.length - 1].date} (${quantities.length} days)`);
      this.logger.log(`üìà Prediction: ${prediction} units`);
      this.logger.log(`üìâ Trend: ${trend} (${trendPercent.toFixed(1)}%)`);

      return {
        success: true,
        nextDayPrediction: prediction,
        nextDate: nextDate.toISOString().split('T')[0],
        confidence,
        inputPeriod: {
          from: sortedSales[0].date,
          to: sortedSales[sortedSales.length - 1].date,
          days: quantities.length,
        },
        trend,
        trendPercent: parseFloat(trendPercent.toFixed(2)),
        statistics: {
          mean: Math.round(mean),
          stdDev: Math.round(stdDev),
          min: Math.min(...quantities),
          max: Math.max(...quantities),
        },
      };
    } catch (error) {
      this.logger.error('‚ùå Next day prediction error:', error);
      throw new InternalServerErrorException('Failed to predict next day: ' + error.message);
    }
  }

  /**
   * Chat-based prediction: Extract data from natural language and predict
   */
  async chatPredict(dto: ChatPredictDto) {
    try {
      const { prompt, language = 'vi' } = dto;

      this.logger.log(`üí¨ Chat prediction request: ${prompt.substring(0, 100)}...`);

      // Call AI to extract data from prompt
      const extractionResult = await this.extractDataFromPrompt(prompt, language);

      if (!extractionResult.success || !extractionResult.quantities || extractionResult.quantities.length === 0) {
        return {
          success: false,
          message: language === 'vi' 
            ? '‚ùå Kh√¥ng th·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ prompt. Vui l√≤ng cung c·∫•p d·ªØ li·ªáu s·ªë l∆∞·ª£ng r√µ r√†ng h∆°n.'
            : '‚ùå Could not extract data from prompt. Please provide clearer quantity data.',
          extractedData: extractionResult,
        };
      }

      // Prepare data for prediction
      const dailySales = extractionResult.quantities.map((quantity, index) => {
        // Generate dates if not provided
        let date: string;
        if (extractionResult.dates && extractionResult.dates[index]) {
          date = extractionResult.dates[index];
        } else {
          // Generate sequential dates from today backwards
          const d = new Date();
          d.setDate(d.getDate() - (extractionResult.quantities.length - 1 - index));
          date = d.toISOString().split('T')[0];
        }
        return { date, quantity };
      });

      // Make prediction
      const predictionDto: PredictNextDayDto = { dailySales };
      const prediction = await this.predictNextDay(predictionDto);

      // Generate natural language response
      const response = this.generateNaturalResponse(prediction, extractionResult, language);

      return {
        success: true,
        message: response,
        prediction: {
          nextDayPrediction: prediction.nextDayPrediction,
          nextDate: prediction.nextDate,
          confidence: prediction.confidence,
          trend: prediction.trend,
          trendPercent: prediction.trendPercent,
        },
        extractedData: {
          quantities: extractionResult.quantities,
          dates: dailySales.map(d => d.date),
          count: extractionResult.quantities.length,
        },
        statistics: prediction.statistics,
      };
    } catch (error) {
      this.logger.error('‚ùå Chat prediction error:', error);
      throw new InternalServerErrorException('Failed to process chat prediction: ' + error.message);
    }
  }

  /**
   * Extract data from natural language prompt using AI
   */
  private async extractDataFromPrompt(prompt: string, language: string): Promise<any> {
    const systemPrompt = language === 'vi' 
      ? `B·∫°n l√† tr·ª£ l√Ω AI chuy√™n tr√≠ch xu·∫•t d·ªØ li·ªáu s·ªë t·ª´ vƒÉn b·∫£n ti·∫øng Vi·ªát.

Nhi·ªám v·ª•: Tr√≠ch xu·∫•t c√°c s·ªë l∆∞·ª£ng b√°n h√†ng t·ª´ prompt c·ªßa user.

Tr·∫£ v·ªÅ JSON v·ªõi format:
{
  "success": true,
  "quantities": [1000, 1200, 1500, ...],
  "dates": ["2024-01-01", "2024-01-02", ...] (optional, n·∫øu c√≥ trong prompt),
  "period": "7 ng√†y" (m√¥ t·∫£ ng·∫Øn),
  "context": "m√¥ t·∫£ ng·∫Øn v·ªÅ d·ªØ li·ªáu"
}

V√≠ d·ª• prompt: "Trong 7 ng√†y qua, doanh s·ªë l√†: 1000, 1200, 1500, 1800, 2000, 2200, 2500"
Output: {"success": true, "quantities": [1000, 1200, 1500, 1800, 2000, 2200, 2500], "period": "7 ng√†y", "context": "doanh s·ªë h√†ng ng√†y"}

N·∫øu kh√¥ng t√¨m th·∫•y s·ªë li·ªáu: {"success": false, "error": "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu s·ªë l∆∞·ª£ng"}`
      : `You are an AI assistant specialized in extracting numerical data from text.

Task: Extract sales quantities from user's prompt.

Return JSON format:
{
  "success": true,
  "quantities": [1000, 1200, 1500, ...],
  "dates": ["2024-01-01", "2024-01-02", ...] (optional),
  "period": "7 days" (brief description),
  "context": "brief description of data"
}

If no data found: {"success": false, "error": "No quantity data found"}`;

    const body = {
      model: this.model,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: prompt },
      ],
      temperature: 0,
      max_tokens: 2000,
      response_format: {
        type: 'json_schema',
        json_schema: {
          name: 'data_extraction',
          strict: true,
          schema: {
            type: 'object',
            properties: {
              success: { type: 'boolean' },
              quantities: { 
                type: 'array', 
                items: { type: 'number' },
                description: 'Array of sales quantities'
              },
              dates: { 
                type: 'array', 
                items: { type: 'string' },
                description: 'Optional array of dates'
              },
              period: { type: 'string', description: 'Time period description' },
              context: { type: 'string', description: 'Brief context' },
              error: { type: 'string', description: 'Error message if failed' },
            },
            required: ['success'],
            additionalProperties: false,
          },
        },
      },
    };

    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(body),
    });

    if (!response.ok) {
      throw new Error(`AI API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    const content = data.choices?.[0]?.message?.content;
    
    if (!content) {
      throw new Error('No response from AI');
    }

    return JSON.parse(content);
  }

  /**
   * Generate natural language response based on prediction
   */
  private generateNaturalResponse(prediction: any, extractedData: any, language: string): string {
    if (language === 'vi') {
      let response = `üìä **Ph√¢n t√≠ch d·ªØ li·ªáu b√°n h√†ng**\n\n`;
      
      if (extractedData.context) {
        response += `D·ªØ li·ªáu: ${extractedData.context}\n`;
      }
      if (extractedData.period) {
        response += `Th·ªùi gian: ${extractedData.period}\n`;
      }
      
      response += `\nüìà **D·ª± ƒëo√°n cho ng√†y ${prediction.nextDate}:**\n`;
      response += `‚Üí S·ªë l∆∞·ª£ng d·ª± ki·∫øn: **${prediction.nextDayPrediction.toLocaleString()} ƒë∆°n v·ªã**\n\n`;
      
      response += `üìâ **Ph√¢n t√≠ch xu h∆∞·ªõng:**\n`;
      if (prediction.trend === 'increasing') {
        response += `‚Üí Xu h∆∞·ªõng: **TƒÇNG** üìà (+${prediction.trendPercent}%)\n`;
      } else if (prediction.trend === 'decreasing') {
        response += `‚Üí Xu h∆∞·ªõng: **GI·∫¢M** üìâ (${prediction.trendPercent}%)\n`;
      } else {
        response += `‚Üí Xu h∆∞·ªõng: **·ªîN ƒê·ªäNH** ‚û°Ô∏è\n`;
      }
      
      response += `‚Üí ƒê·ªô tin c·∫≠y: **${prediction.confidence === 'high' ? 'Cao ‚úÖ' : prediction.confidence === 'medium' ? 'Trung b√¨nh ‚ö†Ô∏è' : 'Th·∫•p ‚ùå'}**\n\n`;
      
      response += `üìä **Th·ªëng k√™:**\n`;
      response += `‚Üí Trung b√¨nh: ${prediction.statistics.mean.toLocaleString()} ƒë∆°n v·ªã\n`;
      response += `‚Üí Cao nh·∫•t: ${prediction.statistics.max.toLocaleString()} ƒë∆°n v·ªã\n`;
      response += `‚Üí Th·∫•p nh·∫•t: ${prediction.statistics.min.toLocaleString()} ƒë∆°n v·ªã\n`;
      
      return response;
    } else {
      let response = `üìä **Sales Data Analysis**\n\n`;
      
      if (extractedData.context) {
        response += `Data: ${extractedData.context}\n`;
      }
      if (extractedData.period) {
        response += `Period: ${extractedData.period}\n`;
      }
      
      response += `\nüìà **Prediction for ${prediction.nextDate}:**\n`;
      response += `‚Üí Expected quantity: **${prediction.nextDayPrediction.toLocaleString()} units**\n\n`;
      
      response += `üìâ **Trend Analysis:**\n`;
      if (prediction.trend === 'increasing') {
        response += `‚Üí Trend: **INCREASING** üìà (+${prediction.trendPercent}%)\n`;
      } else if (prediction.trend === 'decreasing') {
        response += `‚Üí Trend: **DECREASING** üìâ (${prediction.trendPercent}%)\n`;
      } else {
        response += `‚Üí Trend: **STABLE** ‚û°Ô∏è\n`;
      }
      
      response += `‚Üí Confidence: **${prediction.confidence === 'high' ? 'High ‚úÖ' : prediction.confidence === 'medium' ? 'Medium ‚ö†Ô∏è' : 'Low ‚ùå'}**\n\n`;
      
      response += `üìä **Statistics:**\n`;
      response += `‚Üí Average: ${prediction.statistics.mean.toLocaleString()} units\n`;
      response += `‚Üí Maximum: ${prediction.statistics.max.toLocaleString()} units\n`;
      response += `‚Üí Minimum: ${prediction.statistics.min.toLocaleString()} units\n`;
      
      return response;
    }
  }

  /**
   * Get model information
   */
  getModelInfo() {
    const modelExists = fs.existsSync(this.modelPath);
    const preprocessorExists = fs.existsSync(this.preprocessorPath);

    if (!this.session) {
      return {
        loaded: false,
        modelExists,
        preprocessorExists,
        modelPath: this.modelPath,
        preprocessorPath: this.preprocessorPath,
        message: modelExists 
          ? 'Model file found but failed to load. Check server logs.'
          : 'ONNX model not found. Please export your PyTorch model to ONNX format.',
      };
    }

    return {
      loaded: true,
      modelPath: this.modelPath,
      modelExists,
      preprocessorExists,
      preprocessor: this.preprocessor,
      inputNames: this.session.inputNames,
      outputNames: this.session.outputNames,
      message: 'ONNX model loaded and ready for inference',
    };
  }
}
